





#
# Cluster description
#
cluster {
    products {
      CDH: 6.1
      CDSW: 1.5.0
      Anaconda: 5.1.0.1
    }

    parcelRepositories: [ "https://archive.cloudera.com/cdh6/6.1/parcels/",
                         "https://archive.cloudera.com/cdsw1/1.5.0/parcels/",
                         "https://repo.continuum.io/pkgs/misc/parcels/archive/",]

    services: [HDFS, YARN, ZOOKEEPER, CDSW, SPARK_ON_YARN, HIVE, KAFKA, KUDU, IMPALA, HUE]

    configs {
        CDSW {
            "cdsw.domain.config": cdsw.placeholder-domain.com # The fully qualified domain name for the CDSW host
        }
        KAFKA {
            "offsets.topic.replication.factor": 1
            "default.replication.factor" : 1
            "broker_max_heap_size" : 256
        }
        KUDU {
            "default_num_replicas": 1
        }
    }

    masters {
        count: 1
        instance: ${instances.master} {
            tags: {
                application: "CDH 6.1"
                group: masters
            }
        }

        roles {
            HDFS: [NAMENODE, SECONDARYNAMENODE]
            YARN: [RESOURCEMANAGER, JOBHISTORY]
            SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER]
            KAFKA: [KAFKA_BROKER, KAFKA_MIRROR_MAKER]
            KUDU: [KUDU_MASTER]
            IMPALA: [CATALOGSERVER, STATESTORE]
            HUE: [HUE_SERVER]
            HIVE: [HIVESERVER2, HIVEMETASTORE]
         }
         configs {
            HDFS {
                NAMENODE {
                    namenode_log_dir: /data0/log/hadoop-hdfs
                    dfs_name_dir_list: /data1/dfs/nn
                }
                SECONDARYNAMENODE {
                    secondarynamenode_log_dir: /data0/log/hadoop-hdfs
                    fs_checkpoint_dir_list: /data2/dfs/snn
                }
            }
            YARN {
              RESOURCEMANAGER {
                resource_manager_log_dir: /data0/log/hadoop-yarn
              }
              JOBHISTORY {
                  mr2_jobhistory_log_dir: /data0/log/hadoop-mapreduce
              }
            }
            SPARK_ON_YARN {
                SPARK_YARN_HISTORY_SERVER {
                    log_dir: /data0/log/spark
                }
            }
            KUDU {
                    KUDU_MASTER {
                      # The master rarely performs IO. If fs_data_dirs is unset, it will
                      # use the same directory as fs_wal_dir
                      fs_wal_dir: "/data0/kudu/masterwal"
                      fs_data_dirs: "/data1/kudu/master"
                    }
            }
        }
    }

    workers {
        count: 3
        minCount: 3
        instance: ${instances.worker} {
            tags: {
                application: "CDH 6.1"
                group: workers
            }
        }
        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            ZOOKEEPER: [SERVER]
            IMPALA: [IMPALAD]
            KUDU: [KUDU_TSERVER]
        }
        configs {
            HDFS {
                DATANODE {
                    datanode_log_dir: /data0/log/hadoop-hdfs
                    dfs_data_dir_list: "/data2/dfs/dn,/data3/dfs/dn"
                    dfs_datanode_failed_volumes_tolerated: 1
                }
            }
            YARN {
                NODEMANAGER {
                    node_manager_log_dir: /data0/log/hadoop-yarn
                    yarn_nodemanager_log_dirs: "/data2/log/hadoop-yarn/container,/data3/log/hadoop-yarn/container"
                    yarn_nodemanager_local_dirs: "/data2/yarn,/data3/yarn"
                }
            }
            ZOOKEEPER {
                SERVER {
                    zk_server_log_dir: /data0/log/zookeeper
                    dataDir: /data1/zookeeper
                    dataLogDir: /data1/zookeeper
                    maxClientCnxns: 1024
                }
            }
            KUDU {
                    KUDU_TSERVER {
                      # Set fs_wal_dir to an SSD drive (if exists) for better performance.
                      # Set fs_data_dirs to a comma-separated string containing all remaining
                      # disk drives, solid state or otherwise.
                      # If there are multiple drives in the machine, it's best to ensure that
                      # the WAL directory is not located on the same disk as a tserver data
                      # directory.
                      fs_wal_dir: "/data0/kudu/tabletwal"
                      fs_data_dirs: "/data1/kudu/tablet"
                    }
            }
        }
    }

    cdswmasters {
        count: 1
        minCount: 1
        instance: ${instances.cdswmaster} {
            tags: {
                application: "CDH 6.1 + CDSW 1.5"
                group: cdswmasters
            }
        }

        roles {
            HDFS: [GATEWAY]
            YARN: [GATEWAY]
            HIVE: [GATEWAY]
            KAFKA: [GATEWAY]
            SPARK_ON_YARN: [GATEWAY]
            CDSW: [CDSW_MASTER, CDSW_APPLICATION, CDSW_DOCKER]
        }

        configs {
            SPARK_ON_YARN {
                GATEWAY {
                    "spark-conf/spark-env.sh_client_config_safety_valve": """
                      if [ -z ${PYSPARK_PYTHON} ]; then
                        export PYSPARK_PYTHON=/opt/cloudera/parcels/Anaconda/bin/python spark-submit pyspark_script.py;
                      fi
                    """
                }
            }
            CDSW {
                CDSW_DOCKER {
                    "cdsw.docker.devices.config": "/dev/disk/azure/scsi1/lun0 /dev/disk/azure/scsi1/lun1" # related to the data disk configuration
                }
            }
        }
    }


    cdswworkers {
        count: 1
        minCount:1
        instance: ${instances.cdswworker} {
            tags: {
                application: "CDH 6.1 + CDSW 1.5"
                group: cdswworkers
            }
        }

        roles {
            HDFS: [GATEWAY]
            YARN: [GATEWAY]
            HIVE: [GATEWAY]
            KAFKA: [GATEWAY]
            SPARK_ON_YARN: [GATEWAY]
            CDSW: [CDSW_WORKER, CDSW_DOCKER]
        }

        configs {
            CDSW {
                CDSW_DOCKER {
                    "cdsw.docker.devices.config": "/dev/disk/azure/scsi1/lun0 /dev/disk/azure/scsi1/lun1" # related to the data disk configuration
                }
            }
        }
    }

    instancePostCreateScriptsPaths:  ["scripts/nip-io-trick.sh"]

    administrationSettings {
        autoRepairEnabled: true
        autoRepairCooldownPeriodInSeconds: 300
    }

    postCreateScriptsPaths: ["scripts/create-hdfs-folders.sh"]

}
