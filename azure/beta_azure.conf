# Keep this line to enable the VIM syntax highlighting
include "SECRET"

environmentName: env-1
deploymentName: cm-1
name: cluster-1

provider {
    type: azure
    region: "eastus"
    azureCloudEnvironment: "azure"
    subscriptionId: "sxxx"
    tenantId: "txxx"
    clientId: "cxxx"
    clientSecret: ${CLIENTSECRET} 
}

ssh {
    username: "centos"
    privateKey: "azure/azurekey"
}

instances {
    base {
        type: STANDARD_DS15_V2
        image: cloudera-centos-75-latest
        computeResourceGroup: "nyc1-rg"
        networkSecurityGroupResourceGroup: "nyc1-rg"
        networkSecurityGroup: "director-dir-sg"
        virtualNetworkResourceGroup: "nyc1-rg"
        virtualNetwork: "directorvnet"
        subnetName: "default"
        publicIP: No
        hostFqdnSuffix: "mydnsdomain"
        storageAccountType: Premium_LRS
        managedDisks: Yes
	withAcceleratedNetworking: Yes
        bootstrapScripts: [ ${bootstrap-script.install-openjdk8}]
        bootstrapScriptsPaths: [
	       "scripts/azure-os-generic.sh",
	       "scripts/create-log-dir.sh",
               "scripts/clean-yum-cache.sh",
	       "scripts/install-kerberos-client.sh",
	       "scripts/create-users.sh"
        ]
    }

    master: ${instances.base} {
        availabilitySet: "asmaster"
        instanceNamePrefix: master-${name}
        dataDiskCount: 3
        dataDiskSize: 512
        publicIP: Yes
    }

    worker: ${instances.base} {
        availabilitySet: "asworker"
        instanceNamePrefix: worker-${name}
        dataDiskCount: 4
        dataDiskSize: 1024
    }

    edge: ${instances.base} {
        availabilitySet: "asedge"
        instanceNamePrefix: edge-${name}
        dataDiskCount: 1
        dataDiskSize: 512
    }

    cdswmaster: ${instances.base} {
        availabilitySet: "asmaster"
        instanceNamePrefix: cmaster-${name}
         # Use an empty subnet dedicated to the cdsw master node to get predictable IP
         # that can be registered in DNS. Example, if subnet is 10.6.1.0/29 (3
         # available addresses in Azure) the master node will be assigned 10.6.1.4
         # See http://tiny.cloudera.com/azure-available-ip for details.
        subnetName: "default"
        dataDiskCount: 3
        dataDiskSize: 1024
        publicIP: Yes
        normalizationConfig {
            mountAllUnmountedDisks: false
        }
        bootstrapScripts: [ ${bootstrap-script.setupcdswmount}, ${bootstrap-script.install-openjdk8}]
    }

    cdswworker: ${instances.base} {
        availabilitySet: "asworker"
        instanceNamePrefix: cworker-${name}
        dataDiskCount: 2
        dataDiskSize: 1024
        normalizationConfig {
            mountAllUnmountedDisks: false
        }
    }
}

#
# Cloudera Manager + Kerberos
#
cloudera-manager {
    instance: ${instances.edge} {
        tags {
            application: "Cloudera Manager 6.3.0"
        }
    }

    repository: "https://archive.cloudera.com/cm6/6.3.0/redhat7/yum/"
    repositoryKeyUrl: "https://archive.cloudera.com/cm6/6.3.0/redhat7/yum/RPM-GPG-KEY-cloudera"

    krbAdminUsername: "cm/admin@CLOUDERA"
    krbAdminPassword: "Passw0rd!"

    configs {
        CLOUDERA_MANAGER {
            enable_api_debug: true
            custom_banner_html: "Managed by Cloudera Altus Director"

            KDC_TYPE: "MIT KDC"
            AD_KDC_DOMAIN: "cloudera"
            KDC_HOST: "10.4.0.4" # Using IP because of DSE-1796
            SECURITY_REALM: "CLOUDERA"
            KRB_MANAGE_KRB5_CONF: "true"
            KRB_ENC_TYPES: "aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96 arcfour-hmac-md5"

        }
        SERVICEMONITOR {
            mgmt_log_dir:/data0/log/cloudera-scm-firehose
            firehose_storage_dir:/data0/lib/cloudera-service-monitor
        }
        ACTIVITYMONITOR {
            mgmt_log_dir:/data0/log/cloudera-scm-firehose
        }
        HOSTMONITOR {
            mgmt_log_dir: /data0/log/cloudera-scm-firehose
            firehose_storage_dir: /data0/lib/cloudera-host-monitor
        }
        REPORTSMANAGER {
            headlamp_scratch_dir: /data0/lib/cloudera-scm-headlamp
            mgmt_log_dir: /data0/log/cloudera-scm-headlamp
        }
        EVENTSERVER {
            mgmt_log_dir:/data0/log/cloudera-scm-eventserver
            eventserver_index_dir:/data0/lib/cloudera-scm-eventserver
        }
        ALERTPUBLISHER {
            mgmt_log_dir:/data0/log/cloudera-scm-alertpublisher
        }
        NAVIGATOR {
            mgmt_log_dir:/data0/log/cloudera-scm-navigator
        }
        NAVIGATORMETASERVER {
            audit_event_log_dir:/data0/log/cloudera-scm-navigator/audit
            data_dir:/data0/lib/cloudera-scm-navigator
            mgmt_log_dir:/data0/log/cloudera-scm-navigator
        }
    }
    enableEnterpriseTrial: true
    javaInstallationStrategy: NONE

    # Custom service descriptors for CDSW
    csds: [
        "https://archive.cloudera.com/cdsw1/1.6.0/csd/CLOUDERA_DATA_SCIENCE_WORKBENCH-CDH6-1.6.0.jar"
    ]
}

#
# Cluster description
#
cluster {
    products {
      CDH: 6.3
      CDSW: 1.6.0
      Anaconda: 5.1.0.1
    }

    parcelRepositories: [ "https://archive.cloudera.com/cdh6/6.3/parcels/",
                         "https://archive.cloudera.com/cdsw1/1.6.0/parcels/",
                         "https://repo.continuum.io/pkgs/misc/parcels/archive/",]

    services: [HDFS, YARN, ZOOKEEPER, CDSW, SPARK_ON_YARN, OOZIE, HIVE, KAFKA, KUDU, IMPALA, HUE]

    configs {
        CDSW {
            "cdsw.domain.config": cdsw.placeholder-domain.com # The fully qualified domain name for the CDSW host
        }
        KAFKA {
            "offsets.topic.replication.factor": 1
            "default.replication.factor" : 1
        }
    }

    masters {
        count: 1
        instance: ${instances.master} {
            tags: {
                application: "CDH 6.3"
                group: masters
            }
        }

        roles {
            HDFS: [NAMENODE, SECONDARYNAMENODE]
            YARN: [RESOURCEMANAGER, JOBHISTORY]
            SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER]
            KAFKA: [KAFKA_BROKER]
            KUDU: [KUDU_MASTER]
            IMPALA: [CATALOGSERVER, STATESTORE]
            HUE: [HUE_SERVER]
            HIVE: [HIVESERVER2, HIVEMETASTORE]
            OOZIE: [OOZIE_SERVER]
         }
         configs {
            HDFS {
                NAMENODE {
                    namenode_log_dir: /data0/log/hadoop-hdfs
                    dfs_name_dir_list: /data1/dfs/nn
                }
                SECONDARYNAMENODE {
                    secondarynamenode_log_dir: /data0/log/hadoop-hdfs
                    fs_checkpoint_dir_list: /data2/dfs/snn
                }
            }
            YARN {
              RESOURCEMANAGER {
                resource_manager_log_dir: /data0/log/hadoop-yarn
              }
              JOBHISTORY {
                  mr2_jobhistory_log_dir: /data0/log/hadoop-mapreduce
              }
            }
            SPARK_ON_YARN {
                SPARK_YARN_HISTORY_SERVER {
                    log_dir: /data0/log/spark
                }
            }
            KUDU {
                    KUDU_MASTER {
                      # The master rarely performs IO. If fs_data_dirs is unset, it will
                      # use the same directory as fs_wal_dir
                      fs_wal_dir: "/data0/kudu/masterwal"
                      fs_data_dirs: "/data1/kudu/master"
                      default_num_replicas: 1
                    }
            }
            KAFKA {
                KAFKA_BROKER {
                    broker_max_heap_size : 512
                }
            }
        }
    }

    workers {
        count: 3
        minCount: 3
        instance: ${instances.worker} {
            tags: {
                application: "CDH 6.3"
                group: workers
            }
        }
        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            ZOOKEEPER: [SERVER]
            IMPALA: [IMPALAD]
            KUDU: [KUDU_TSERVER]
            HIVE: [GATEWAY]
            SPARK_ON_YARN: [GATEWAY]
        }
        configs {
            HDFS {
                DATANODE {
                    datanode_log_dir: /data0/log/hadoop-hdfs
                    dfs_data_dir_list: "/data2/dfs/dn,/data3/dfs/dn"
                    dfs_datanode_failed_volumes_tolerated: 1
                }
            }
            YARN {
                NODEMANAGER {
                    node_manager_log_dir: /data0/log/hadoop-yarn
                    yarn_nodemanager_log_dirs: "/data2/log/hadoop-yarn/container,/data3/log/hadoop-yarn/container"
                    yarn_nodemanager_local_dirs: "/data2/yarn,/data3/yarn"
                }
            }
            ZOOKEEPER {
                SERVER {
                    zk_server_log_dir: /data0/log/zookeeper
                    dataDir: /data1/zookeeper
                    dataLogDir: /data1/zookeeper
                    maxClientCnxns: 1024
                }
            }
            KUDU {
                    KUDU_TSERVER {
                      # Set fs_wal_dir to an SSD drive (if exists) for better performance.
                      # Set fs_data_dirs to a comma-separated string containing all remaining
                      # disk drives, solid state or otherwise.
                      # If there are multiple drives in the machine, it's best to ensure that
                      # the WAL directory is not located on the same disk as a tserver data
                      # directory.
                      fs_wal_dir: "/data0/kudu/tabletwal"
                      fs_data_dirs: "/data1/kudu/tablet"
                    }
            }
        }
    }

    cdswmasters {
        count: 1
        minCount: 1
        instance: ${instances.cdswmaster} {
            tags: {
                application: "CDH 6.3 + CDSW 1.6"
                group: cdswmasters
            }
        }

        roles {
            HDFS: [GATEWAY]
            YARN: [GATEWAY]
            HIVE: [GATEWAY]
            KAFKA: [GATEWAY]
            SPARK_ON_YARN: [GATEWAY]
            CDSW: [CDSW_MASTER, CDSW_APPLICATION, CDSW_DOCKER]
        }

        configs {
            SPARK_ON_YARN {
                GATEWAY {
                    "spark-conf/spark-env.sh_client_config_safety_valve": """
                      if [ -z ${PYSPARK_PYTHON} ]; then
                        export PYSPARK_PYTHON=/opt/cloudera/parcels/Anaconda/bin/python spark-submit pyspark_script.py;
                      fi
                    """
                }
            }
            CDSW {
                CDSW_DOCKER {
                    "cdsw.docker.devices.config": "/dev/disk/azure/scsi1/lun0 /dev/disk/azure/scsi1/lun1" # related to the data disk configuration
                }
            }
        }
    }


    cdswworkers {
        count: 2
        minCount: 2
        instance: ${instances.cdswworker} {
            tags: {
                application: "CDH 6.3 + CDSW 1.6"
                group: cdswworkers
            }
        }

        roles {
            HDFS: [GATEWAY]
            YARN: [GATEWAY]
            HIVE: [GATEWAY]
            KAFKA: [GATEWAY]
            SPARK_ON_YARN: [GATEWAY]
            CDSW: [CDSW_WORKER, CDSW_DOCKER]
        }

        configs {
            CDSW {
                CDSW_DOCKER {
                    "cdsw.docker.devices.config": "/dev/disk/azure/scsi1/lun0 /dev/disk/azure/scsi1/lun1" # related to the data disk configuration
                }
            }
        }
    }

    instancePostCreateScriptsPaths:  ["scripts/nip-io-trick.sh"]

    administrationSettings {
        autoRepairEnabled: true
        autoRepairCooldownPeriodInSeconds: 300
    }

    postCreateScriptsPaths: ["scripts/create-hdfs-folders.sh"]

}

#
# Bootstrap scripts
#

bootstrap-script {
install-openjdk8:
"""#!/bin/sh
yum install -y java-1.8.0-openjdk-devel
""",
setupcdswmount: """#!/bin/sh
set -e
# Mount one volume for application data
device="/dev/disk/azure/scsi1/lun2"
mount="/var/lib/cdsw"

echo "Making file system"
mkfs.ext4 -F -E lazy_itable_init=1 "$device" -m 0

echo "Mounting $device on $mount"
if [ ! -e "$mount" ]; then
  mkdir -p "$mount"
fi

mount -o defaults,noatime "$device" "$mount"
echo "$device $mount ext4 defaults,noatime 0 0" >> /etc/fstab

exit 0
"""
}
