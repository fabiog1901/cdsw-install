# Keep this line to enable the VIM syntax
include "SECRET"

name: cluster-1
environmentName: env-1
deploymentName: cm-1

provider {
    type: azure
    region: "eastus"
    azureCloudEnvironment: "azure"
    subscriptionId: "1088c520-4023-4865-b663-2ae05be69a5f"
    tenantId: "10a087a4-0454-49d4-bac1-caf7712c924f"
    clientId: "0caf6a5f-06c1-4299-a2b9-f6ee0d27c6a8"
    clientSecret: ${CLIENTSECRET} 
}

ssh {
    username: "centos"
    privateKey: "azure/azurekey"
}

instances {
    base {
        type: STANDARD_DS12_V2
        image: cloudera-centos-74-latest
        computeResourceGroup: "nyc1-rg"
        networkSecurityGroupResourceGroup: "nyc1-rg"
        networkSecurityGroup: "director-dir-sg"
        virtualNetworkResourceGroup: "nyc1-rg"
        virtualNetwork: "directorvnet"
        subnetName: "default"
        publicIP: No
        hostFqdnSuffix: "mydnsdomain"
        storageAccountType: Premium_LRS
        managedDisks: Yes
        bootstrapScriptsPaths: [
	       "scripts/azure-os-generic.sh",
	       "scripts/create-log-dir.sh",
               "scripts/install-java8.sh",
               "scripts/clean-yum-cache.sh",
	       "scripts/install-kerberos-client.sh",
	       "scripts/create-users.sh"
        ]
    }

    master: ${instances.base} {
        availabilitySet: "asmaster"
        instanceNamePrefix: master-${name}
        dataDiskCount: 3
        dataDiskSize: 512
        publicIP: Yes
    }

    worker: ${instances.base} {
        availabilitySet: "asworker"
        instanceNamePrefix: worker-${name}
        dataDiskCount: 4
        dataDiskSize: 1024
    }

    edge: ${instances.base} {
        availabilitySet: "asedge"
        instanceNamePrefix: edge-${name}
        dataDiskCount: 1
        dataDiskSize: 512
    }

    cdswmaster: ${instances.base} {
        availabilitySet: "asmaster"
        instanceNamePrefix: cmaster-${name}
         # Use an empty subnet dedicated to the cdsw master node to get predictable IP
         # that can be registered in DNS. Example, if subnet is 10.6.1.0/29 (3
         # available addresses in Azure) the master node will be assigned 10.6.1.4
         # See http://tiny.cloudera.com/azure-available-ip for details.
        subnetName: "default"
        dataDiskCount: 3
        dataDiskSize: 1024
        publicIP: Yes
        normalizationConfig {
            mountAllUnmountedDisks: false
        }
        bootstrapScripts: [ ${bootstrap-script.setupcdswmount}]
    }

    cdswworker: ${instances.base} {
        availabilitySet: "asworker"
        instanceNamePrefix: cworker-${name}
        dataDiskCount: 2
        dataDiskSize: 1024
        normalizationConfig {
            mountAllUnmountedDisks: false
        }
    }
}


cloudera-manager {

    instance: ${instances.edge} {
        tags {
            application: "Cloudera Manager 5"
        }
    }

    # Support for CDSW requires Cloudera Manager 5.13.1+
    repository: "https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5.15/"
    repositoryKeyUrl: "https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/RPM-GPG-KEY-cloudera"
    
    krbAdminUsername: "cm/admin@HADOOP.LOCAL"
    krbAdminPassword: "Passw0rd!"
    
    configs {
        CLOUDERA_MANAGER {
            enable_api_debug: true
            custom_banner_html: "Managed by Cloudera Altus Director"
            
            KDC_TYPE: "MIT KDC"
	    AD_KDC_DOMAIN: ""
	    KDC_HOST: "10.1.0.4" # Using IP because of DSE-1796
	    SECURITY_REALM: "HADOOP.LOCAL" 
	    KRB_MANAGE_KRB5_CONF: "true"
            KRB_ENC_TYPES: "aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96 arcfour-hmac-md5"
            
        }
        SERVICEMONITOR {
            mgmt_log_dir:/data0/log/cloudera-scm-firehose
            firehose_storage_dir:/data0/lib/cloudera-service-monitor
        }
        ACTIVITYMONITOR {
            mgmt_log_dir:/data0/log/cloudera-scm-firehose
        }
        HOSTMONITOR {
            mgmt_log_dir: /data0/log/cloudera-scm-firehose
            firehose_storage_dir: /data0/lib/cloudera-host-monitor
        }
        REPORTSMANAGER {
            headlamp_scratch_dir: /data0/lib/cloudera-scm-headlamp
            mgmt_log_dir: /data0/log/cloudera-scm-headlamp
        }
        EVENTSERVER {
            mgmt_log_dir:/data0/log/cloudera-scm-eventserver
            eventserver_index_dir:/data0/lib/cloudera-scm-eventserver
        }
        ALERTPUBLISHER {
            mgmt_log_dir:/data0/log/cloudera-scm-alertpublisher
        }
        NAVIGATOR {
            mgmt_log_dir:/data0/log/cloudera-scm-navigator
        }
        NAVIGATORMETASERVER {
            audit_event_log_dir:/data0/log/cloudera-scm-navigator/audit
            data_dir:/data0/lib/cloudera-scm-navigator
            mgmt_log_dir:/data0/log/cloudera-scm-navigator
        }
    }
    enableEnterpriseTrial: true
    javaInstallationStrategy: NONE

    # Custom service descriptors for CDSW and Spark2
    csds: [
        "https://archive.cloudera.com/cdsw1/1.4.2/csd/CLOUDERA_DATA_SCIENCE_WORKBENCH-CDH5-1.4.2.jar",
        "https://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.3.0.cloudera4.jar"
    ]
}

#
# Cluster description
#
cluster {
    products {
      CDH: 5.15
      CDSW: 1.4.2
      SPARK2: 2
    }

    parcelRepositories: [ "https://archive.cloudera.com/cdh5/parcels/5.15/",
                         "https://archive.cloudera.com/cdsw1/1.4.2/parcels/",
                         "https://archive.cloudera.com/spark2/parcels/2.3.0.cloudera4/"]

    services: [HDFS, YARN, ZOOKEEPER, CDSW, SPARK2_ON_YARN]

    configs {
        CDSW {
            "cdsw.domain.config": cdsw.placeholder-domain.com # The fully qualified domain name for the CDSW host
        }
    }

    masters {
        count: 1
        instance: ${instances.master} {
            tags: {
                application: "CDH 5.15"
                group: masters
            }
        }

        roles {
            HDFS: [NAMENODE, SECONDARYNAMENODE]
            YARN: [RESOURCEMANAGER, JOBHISTORY]
            SPARK2_ON_YARN: [SPARK2_YARN_HISTORY_SERVER]
         }
         configs {
            HDFS {
                NAMENODE {
                    namenode_log_dir: /data0/log/hadoop-hdfs
                    dfs_name_dir_list: /data1/dfs/nn
                }
                SECONDARYNAMENODE {
                    secondarynamenode_log_dir: /data0/log/hadoop-hdfs
                    fs_checkpoint_dir_list: /data2/dfs/snn
                }
            }
            YARN {
              RESOURCEMANAGER {
                resource_manager_log_dir: /data0/log/hadoop-yarn
              }
              JOBHISTORY {
                  mr2_jobhistory_log_dir: /data0/log/hadoop-mapreduce
              }
            }
            SPARK2_ON_YARN {
                SPARK2_YARN_HISTORY_SERVER {
                    log_dir: /data0/log/spark2
                }
            }
        }
    }

    workers {
        count: 3
        minCount: 3
        instance: ${instances.worker} {
            tags: {
                application: "CDH 5.15"
                group: workers
            }
        }
        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            ZOOKEEPER: [SERVER]
        }
        configs {
            HDFS {
                DATANODE {
                    datanode_log_dir: /data0/log/hadoop-hdfs
                    dfs_data_dir_list: "/data2/dfs/dn,/data3/dfs/dn"
                    dfs_datanode_failed_volumes_tolerated: 1
                }
            }
            YARN {
                NODEMANAGER {
                    node_manager_log_dir: /data0/log/hadoop-yarn
                    yarn_nodemanager_log_dirs: "/data2/log/hadoop-yarn/container,/data3/log/hadoop-yarn/container"
                    yarn_nodemanager_local_dirs: "/data2/yarn,/data3/yarn"
                }
            }
            ZOOKEEPER {
                SERVER {
                    zk_server_log_dir: /data0/log/zookeeper
                    dataDir: /data1/zookeeper
                    dataLogDir: /data1/zookeeper
                    maxClientCnxns: 1024
                }
            }
        }
    }

    cdswmasters {
        count: 1
        instance: ${instances.cdswmaster} {
            tags: {
                application: "CDH 5.15 + CDSW 1.4.2"
                group: cdswmasters
            }
        }

        roles {
            HDFS: [GATEWAY]
            YARN: [GATEWAY]
            CDSW: [CDSW_MASTER, CDSW_APPLICATION, CDSW_DOCKER]
            SPARK2_ON_YARN: [GATEWAY]
        }

        configs {
            CDSW {
                CDSW_DOCKER {
                    "cdsw.docker.devices.config": "/dev/disk/azure/scsi1/lun0 /dev/disk/azure/scsi1/lun1" # related to the data disk configuration
                }
            }
        }
    }

    cdswworkers {
        count: 2
        minCount: 2
        instance: ${instances.cdswworker} {
            tags: {
                application: "CDH 5.15 + CDSW 1.4.2"
                group: cdswworkers
            }
        }

        roles {
            HDFS: [GATEWAY]
            YARN: [GATEWAY]
            CDSW: [CDSW_WORKER, CDSW_DOCKER]
            SPARK2_ON_YARN: [GATEWAY]
        }

        configs {
            CDSW {
                CDSW_DOCKER {
                    "cdsw.docker.devices.config": "/dev/disk/azure/scsi1/lun0 /dev/disk/azure/scsi1/lun1" # related to the data disk configuration
                }
            }
        }
    }
    
    instancePostCreateScriptsPaths:  ["scripts/nip-io-trick.sh"]
    
    administrationSettings {
	autoRepairEnabled: true
	autoRepairCooldownPeriodInSeconds: 300
    }
        
    postCreateScriptsPaths: ["scripts/create-hdfs-folders.sh"]
    
}

#
# Bootstrap scripts
#

bootstrap-script {
   setupcdswmount: """#!/bin/sh
set -e
# Mount one volume for application data
device="/dev/disk/azure/scsi1/lun2"
mount="/var/lib/cdsw"

echo "Making file system"
mkfs.ext4 -F -E lazy_itable_init=1 "$device" -m 0

echo "Mounting $device on $mount"
if [ ! -e "$mount" ]; then
  mkdir -p "$mount"
fi

mount -o defaults,noatime "$device" "$mount"
echo "$device $mount ext4 defaults,noatime 0 0" >> /etc/fstab

exit 0
"""
}
